# 분석 에이전트

## 개요

분석 에이전트는 RAG(Retrieval-Augmented Generation) 기반의 문서 분석 에이전트로, 관련 문서에서 정확한 인용을 포함한 증거 기반의 답변을 제공합니다.

## 작동 방식

1. **관련 청크 검색**: 선택된 문서를 기반으로 ChromaDB에서 관련 청크를 추출합니다.
2. **메타데이터 추가**: PostgreSQL에서 페이지 번호와 메타데이터를 추가합니다.
3. **증거 기반 답변 생성**: LLM을 사용하여 인용이 포함된 답변을 생성합니다.
4. **정확한 출처 제공**: 문서 제목, 페이지 번호, 텍스트 발췌를 출력합니다.

## 사용 기술 및 도구

- **ChromaDB**: 문서 청크 저장 및 검색.
- **PostgreSQL**: 페이지 번호와 같은 메타데이터 저장.
- **LLM 서비스**: 증거 기반 답변 생성.
- **ReAct Reasoning Tool**: 품질 게이트 확인 및 증거 항목 생성.
- **SQLAlchemy**: 데이터베이스 상호작용.

## 주요 구성 요소

- **스키마(Schemas)**: 요청 및 응답 구조 정의.
- **프롬프트(Prompts)**: 시스템 및 분석 프롬프트 포함.
- **서비스(Services)**: LLM 및 임베딩 서비스와 통합.
- **데이터베이스 모델**: 문서 및 청크 데이터 관리.

## 주요 파일

- `agent.py`: 분석 에이전트의 주요 구현 파일.

## 평가 방법

### 결과물 평가 기준

1. **인용 정확성 (Citation Accuracy)**
   - 모든 주장에 [논문 제목] 형식의 인용이 포함되어 있는가?
   - 인용된 논문이 실제 선택된 문서와 일치하는가?
   - 평가 지표: 인용 포함률, 인용 정확도

2. **증거 기반 답변 (Evidence-Based Response)**
   - 답변이 검색된 문서 청크의 내용을 기반으로 하는가?
   - 환각(hallucination)이 없이 실제 문서 내용만 사용하는가?
   - 평가 지표: Faithfulness score, 문서 일치도

3. **관련성 (Relevance)**
   - 사용자 질문에 적절한 답변을 제공하는가?
   - 검색된 청크가 질문과 관련이 있는가?
   - 평가 지표: 의미적 유사도, 관련성 점수

4. **완전성 (Completeness)**
   - 질문에 대한 모든 주요 측면을 다루는가?
   - 충분한 컨텍스트와 세부 정보를 제공하는가?
   - 평가 지표: 답변 길이, 커버리지 범위

### 평가 프로세스

```python
# 평가 예시
result = {
    "citation_accuracy": 0.95,  # 95% 인용 정확도
    "faithfulness": 0.92,        # 92% 문서 충실도
    "relevance": 0.88,            # 88% 관련성
    "completeness": 0.85,         # 85% 완전성
    "overall_score": 0.90         # 종합 점수
}
```

### 품질 기준

- **후수 (>90%)**: 모든 인용이 정확하고 증거 기반 답변
- **양호 (70-90%)**: 대부분의 인용이 정확하나 일부 개선 필요
- **개선 필요 (<70%)**: 인용 오류 또는 관련성 부족
